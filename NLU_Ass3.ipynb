{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"ner.txt\", encoding=\"ISO-8859-1\")\n",
    "train_data = []\n",
    "train_sentence = []\n",
    "\n",
    "for l in file:\n",
    "    w = l.split()\n",
    "    if len(w) == 0 or w[0] == '' or w is None:\n",
    "        train_data.append(train_sentence)\n",
    "        train_sentence = []\n",
    "    else:\n",
    "        train_sentence.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_capital_and_digit_tag(train_data):\n",
    "    for sentence_no in range(len(train_data)):\n",
    "        for token_no in range(len(train_data[sentence_no])):\n",
    "            r = re.match(r'[A-Z](.*)', train_data[sentence_no][token_no][0])\n",
    "            d = re.match(r'([0-9]+)$', train_data[sentence_no][token_no][0])\n",
    "            if r is not None:\n",
    "                train_data[sentence_no][token_no].insert(1, 'CAPITAL')\n",
    "            if d is not None:\n",
    "                train_data[sentence_no][token_no].insert(1, 'DIGITS')\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_stop_words(train_data):\n",
    "    for sentence_no in range(len(train_data)):\n",
    "        for token_no in range(len(train_data[sentence_no])):\n",
    "            tag = nltk.word_tokenize(train_data[sentence_no][token_no][0])\n",
    "            if tag[0] in stop_words:\n",
    "                train_data[sentence_no][token_no].insert(1, 'stopwords')\n",
    "    return train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_complete_training_data(train_data):\n",
    "    f = open(\"Generated_feature_Ner.txt\", 'w')\n",
    "    for sentence_no in range(len(train_data)):\n",
    "        for token_no in range(len(train_data[sentence_no])):\n",
    "            token_length = len(train_data[sentence_no][token_no]) - 1\n",
    "            for token in train_data[sentence_no][token_no]:\n",
    "                if token_length == 0:\n",
    "                    f.write(str(token))\n",
    "                else:\n",
    "                    f.write(str(token) + \" \")\n",
    "\n",
    "                token_length -= 1\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_pos_tag(train_data):\n",
    "    for sentence_no in range(len(train_data)):\n",
    "        for token_no in range(len(train_data[sentence_no])):\n",
    "            tag = nltk.tag.pos_tag(list(train_data[sentence_no][token_no][0]))\n",
    "            train_data[sentence_no][token_no].insert(1, tag[0][1])\n",
    "    return train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_wordnet_semantic_label(train_data):\n",
    "    for sentence_no in range(len(train_data)):\n",
    "        for token_no in range(len(train_data[sentence_no])):\n",
    "            syns = wordnet.synsets(train_data[sentence_no][token_no][0])\n",
    "            if len(syns) > 0:\n",
    "                train_data[sentence_no][token_no].insert(1, syns[0].lemmas()[0].name())\n",
    "                syns = ''\n",
    "    return train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "np.random.shuffle(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = add_wordnet_semantic_label(train_data)\n",
    "train_data = find_stop_words(train_data)\n",
    "train_data = add_pos_tag(train_data)\n",
    "train_data = add_capital_and_digit_tag(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_complete_training_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
